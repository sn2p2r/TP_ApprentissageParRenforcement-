x# ğŸ® Projet Apprentissage par Renforcement : DQN vs Monte Carlo

Ce dÃ©pÃ´t implÃ©mente deux mÃ©thodes dâ€™apprentissage par renforcement appliquÃ©es au jeu **Three In Row**.  
Lâ€™objectif de lâ€™agent est dâ€™aligner ses **3 pions bleus** pour gagner la partie.

---

## ğŸ•¹ï¸ Structure du projet

```
manette/
â”‚â”€â”€ env.py            # DÃ©finition de lâ€™environnement ThreeInRowEnv
â”‚â”€â”€ run_all.py        # Menu interactif pour lancer DQN ou Monte Carlo

dqn_project/
â”‚â”€â”€ env.py   
â”‚â”€â”€ train_dqn.py      # EntraÃ®nement avec Deep Q-Network (DQN)
â”‚â”€â”€ play.py           # Launcher Tkinter pour jouer avec le modÃ¨le DQN
â”‚â”€â”€ dqn_model.pth     # ModÃ¨le sauvegardÃ© aprÃ¨s apprentissage

monte_carlo_project/
â”‚â”€â”€ env.py   
â”‚â”€â”€ mc_control.py     # EntraÃ®nement avec Monte Carlo Control
â”‚â”€â”€ mc_play.py        # Launcher Tkinter pour jouer avec la politique MC
â”‚â”€â”€ mc_policy.pkl     # Politique sauvegardÃ©e aprÃ¨s apprentissage
```

---

## ğŸ“– RÃ¨gles du jeu (Three In Row)

1. Plateau **3x3**.  
2. Chaque joueur dispose de **3 pions** en meme temps 
3. **Phase de placement** :  
   - Lâ€™agent place ses 3 pions un par un.  
   - Lâ€™adversaire place aussi 3 pions, automatiquement et alÃ©atoirement.  
4. **Phase de mouvement** :  
   - Lâ€™agent peut dÃ©placer un pion vers une case voisine libre (y compris en diagonale).  
   - Lâ€™adversaire rouge est fixe dans la version actuelle (pions immobiles).  
5. La partie se termine si :  
   - Lâ€™agent aligne 3 pions â†’ **Victoire**.  
   - Aucun coup valide â†’ **Match nul / dÃ©faite**.  

---

## ğŸ“š Partie ThÃ©orique

### ğŸ”¹ Deep Q-Learning (DQN)
- Approximation de la fonction Q(s,a) avec un rÃ©seau de neurones.  
- Utilise **Experience Replay** + **rÃ©seau cible** pour stabiliser lâ€™apprentissage.  
- Politique **epsilon-greedy** pour Ã©quilibrer exploration/exploitation.  

**Avantages :**
- AdaptÃ© aux grands espaces dâ€™Ã©tats.  
- Capable dâ€™apprendre des stratÃ©gies complexes.  
- Bonne gÃ©nÃ©ralisation.  

**InconvÃ©nients :**
- EntraÃ®nement lent (beaucoup dâ€™Ã©pisodes nÃ©cessaires).  
- Sensible aux hyperparamÃ¨tres.  
- Moins interprÃ©table (boÃ®te noire).  

---

### ğŸ”¹ Monte Carlo Control (MC)
- Apprentissage basÃ© sur les **retours dâ€™Ã©pisodes complets**.  
- Politique epsilon-greedy Ã©galement.  

**Avantages :**
- Simple Ã  comprendre et implÃ©menter.  
- ThÃ©oriquement converge avec assez dâ€™Ã©pisodes.  
- Pas besoin de modÃ¨le complexe.  

**InconvÃ©nients :**
- Beaucoup plus lent que DQN pour les grands espaces.  
- Variance Ã©levÃ©e â†’ instable sur peu dâ€™Ã©pisodes.  
- Peu adaptÃ© aux environnements continus.  

---

## ğŸ› ï¸ Partie Pratique

### ğŸ’» PrÃ©-requis
- Python 3.10+  
- Install requirements.txt : pip install -r requirements.txt
- IDE conseillÃ© : **Visual Studio Code (VS Code)** avec terminal intÃ©grÃ©.
  
### ğŸš€ Lancer le menu principal
Depuis **Visual Studio Code (VS Code)**, ouvrez un terminal intÃ©grÃ© et exÃ©cutez :
```bash
cd manette
python run_all.py
```

Vous aurez accÃ¨s au menu :

```
=== Three In Row - Menu ===
1. EntraÃ®ner DQN
2. Jouer avec DQN
3. EntraÃ®ner Monte Carlo
4. Jouer avec Monte Carlo
0. Quitter
```

### ğŸ”¹ EntraÃ®ner un agent
- **DQN** â†’ choix 1  
- **Monte Carlo** â†’ choix 3  

### ğŸ”¹ Jouer avec un agent
- **DQN** â†’ choix 2  
- **Monte Carlo** â†’ choix 4  

Une interface Tkinter sâ€™ouvre avec :  
- Plateau 3x3  
- Pions rouges fixes (adversaire)  
- Pions bleus (contrÃ´lÃ©s par lâ€™agent)  
- Boutons `Jouer un coup` et `Nouvelle partie`.  

---

## âš–ï¸ Comparaison DQN vs Monte Carlo

|                   | DQN âœ… | Monte Carlo âšª |
|--------------------------|--------|----------------|
| Vitesse dâ€™apprentissage | Rapide aprÃ¨s rÃ©glages | Lent |
| ComplexitÃ© implÃ©mentation| Plus complexe (rÃ©seaux, replay buffer) | Simple |
| AdaptÃ© aux grands espaces | Oui | Non |
| StabilitÃ© | Bonne avec rÃ©seau cible | Variable |

---

## ğŸ“ Conclusion
- **Monte Carlo** â†’ idÃ©al pour dÃ©buter, pÃ©dagogie et environnements petits.  
- **DQN** â†’ puissant, gÃ©nÃ©ralisable, adaptÃ© aux environnements plus complexes.  
 

